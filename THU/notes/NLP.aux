\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{word2vec}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}NLP with DL}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:nlp}{{1}{5}{NLP with DL}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Word Vector}{5}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An example of word analogy of man:woman :: king:?}}{5}{figure.1.1}\protected@file@percent }
\newlabel{fig:word_analogy}{{1.1}{5}{Word Vector}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Word2vec}{5}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A demo of the window size and $p(w_o | w_c)$}}{6}{figure.1.2}\protected@file@percent }
\newlabel{fig:word2vec:window_cond}{{1.2}{6}{Word2vec}{figure.1.2}{}}
\newlabel{eq:word2vec_loss}{{1.3}{6}{Word2vec}{equation.1.1.3}{}}
\citation{rohde2005_hacks}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}HW1}{8}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}GloVe}{8}{subsection.1.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces An example of co-occurrence matrix with window size of $1$}}{8}{figure.1.3}\protected@file@percent }
\newlabel{fig:cooccurrence_matrix}{{1.3}{8}{GloVe}{figure.1.3}{}}
\citation{GloVe}
\citation{GloVe}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces An example of the conditional probabilities and their ratio in GloVe paper.}}{9}{figure.1.4}\protected@file@percent }
\newlabel{fig:ratio_cooccurrence}{{1.4}{9}{GloVe}{figure.1.4}{}}
\citation{huang-etal-2012-improving}
\citation{TACL_word_senses}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Word sense ambiguity}{10}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Math Backgrounds}{10}{section.1.2}\protected@file@percent }
\citation{Xavier}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Data Preprocessing}{12}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces An exmaple of mean subtration.}}{12}{figure.1.5}\protected@file@percent }
\newlabel{fig:mean_subtraction}{{1.5}{12}{Data Preprocessing}{figure.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces An example of normalization.}}{12}{figure.1.6}\protected@file@percent }
\newlabel{fig:normalization}{{1.6}{12}{Data Preprocessing}{figure.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Parameter Initialization}{12}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Optimizer}{12}{subsection.1.2.3}\protected@file@percent }
\citation{Adam}
\citation{dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces A picture of momentum.}}{13}{figure.1.7}\protected@file@percent }
\newlabel{fig:momentum}{{1.7}{13}{Optimizer}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Regularization}{13}{subsection.1.2.4}\protected@file@percent }
\citation{BatchNorm}
\citation{NER_ICML}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Practice: Named Entity Recognition}{14}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}HW2}{15}{subsection.1.2.6}\protected@file@percent }
\citation{nivre-2003-efficient}
\citation{nivre-2003-efficient}
\citation{chen-manning-2014-fast}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Dependency Parser}{16}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Neural Dependency Parsing}{16}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}HW3}{17}{subsection.1.3.2}\protected@file@percent }
\newpmemlabel{^_1}{17}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Language Modeling and Recurrent Neural Networks}{18}{section.1.4}\protected@file@percent }
\newpmemlabel{^_2}{18}
\citation{RNN_vanishing}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Principle of RNN}}{19}{figure.1.8}\protected@file@percent }
\newlabel{RNN}{{1.8}{19}{Language Modeling and Recurrent Neural Networks}{figure.1.8}{}}
\citation{LSTM}
\citation{GRU}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces The repeating module in an LSTM contains four interacting layers.}}{20}{figure.1.9}\protected@file@percent }
\newlabel{fig:LSTM}{{1.9}{20}{Language Modeling and Recurrent Neural Networks}{figure.1.9}{}}
\citation{ResNet}
\citation{DenseNet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Residual connections.}}{21}{figure.1.10}\protected@file@percent }
\newlabel{fig:ResNet}{{1.10}{21}{Language Modeling and Recurrent Neural Networks}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Dense Net.}}{21}{figure.1.11}\protected@file@percent }
\newlabel{fig:DenseNet}{{1.11}{21}{Language Modeling and Recurrent Neural Networks}{figure.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Seq2Seq and Attention}{21}{section.1.5}\protected@file@percent }
\citation{SMT}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Alignment from french to english translation.}}{22}{figure.1.12}\protected@file@percent }
\newlabel{fig:alignment}{{1.12}{22}{Seq2Seq and Attention}{figure.1.12}{}}
\newpmemlabel{^_3}{22}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Training phase for NMT.}}{22}{figure.1.13}\protected@file@percent }
\newlabel{fig:NMT_training}{{1.13}{22}{Seq2Seq and Attention}{figure.1.13}{}}
\newpmemlabel{^_4}{23}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Seq2Seq with attention.}}{23}{figure.1.14}\protected@file@percent }
\newlabel{fig:Seq2Seq_Attention}{{1.14}{23}{Seq2Seq and Attention}{figure.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}HW4}{24}{subsection.1.5.1}\protected@file@percent }
\citation{SAR}
\citation{DrQA}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Tips for Research}{26}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Contextual Word Representations and Pretraining}{26}{section.1.6}\protected@file@percent }
\newlabel{contextual_word_rep}{{1.6}{26}{Contextual Word Representations and Pretraining}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Attention Model and Question Answering}{26}{section.1.7}\protected@file@percent }
\citation{BiDAF}
\newpmemlabel{^_5}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Architecture of BiDAF}}{27}{figure.1.15}\protected@file@percent }
\newlabel{fig:bidaf}{{1.15}{27}{Attention Model and Question Answering}{figure.1.15}{}}
\citation{kim-2014-convolutional}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}ConvNets for NLP}{28}{section.1.8}\protected@file@percent }
\newpmemlabel{^_6}{28}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Simple ConvNet for NLP classification.}}{28}{figure.1.16}\protected@file@percent }
\newlabel{fig:ConvNetNLP}{{1.16}{28}{ConvNets for NLP}{figure.1.16}{}}
\citation{VDCNN}
\citation{morphology}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Subword models}{30}{section.1.9}\protected@file@percent }
\@setckpt{NLP}{
\setcounter{page}{31}
\setcounter{equation}{36}
\setcounter{enumi}{4}
\setcounter{enumii}{2}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{27}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{9}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{0}
\setcounter{Item}{23}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{10}
\setcounter{cp@cntr}{6}
\setcounter{NAT@ctr}{0}
\setcounter{@tufte@num@bibkeys}{0}
\setcounter{AM@survey}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{parentequation}{0}
\setcounter{mytheorem}{0}
\setcounter{myproposition}{0}
\setcounter{mydefinition}{0}
\setcounter{mycorollary}{0}
\setcounter{myexample}{0}
\setcounter{myexercise}{0}
\setcounter{Ex}{0}
\setcounter{theorem}{0}
\setcounter{definition}{0}
\setcounter{section@level}{0}
}
