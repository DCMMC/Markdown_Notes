#### tf conv2d_transpose 反卷积

从形状上的变化可以理解为卷积的逆过程，主要是要通过 stride，kernel_size 和 padding 来计算形状变化。

#### tf conv2d 的 data_format

tf 默认的 data_format 是 NHWC （channel last），而 NVIDIA 的 DNN library 推荐 NCHW （channel first）。

**N**: batch

**H**: height (spatial dimension)

**W**: width (spatial dimension)

**C**: number of channels

#### 卷积中形状的计算

按照 [cs231n](https://cs231n.github.io/convolutional-networks/) 我们有以下计算公式：

$W_2 = \frac{W_1 - F_w + 2P}{S_w} +1$

$H_2 = \frac{H_1 - F_h + 2P}{S_h} +1$

其中 $F_w$ 表示卷积核的宽度，$S_w$ 表示水平方向的步长，其他同理，这里 $2P$ 表示两边（上下，或左右）同时 padding 一样的数量。

可是实际过程中，该公式计算结果可能不是整数，所以需要使用 padding 来解决这个问题。

具体以 tf 为例（[参考博客](https://mmuratarat.github.io/2019-01-17/implementing-padding-schemes-of-tensorflow-in-python)），`valid` 模式表示不填充，所以最终的计算公式为：

$H_2 = \lceil \frac{H_1 - F_h + 1}{S_h} \rceil, W_2 = \lceil \frac{W_2-F_w+1}{S_w}\rceil$

`same` 模式表示输出跟输入的形状一样（不考虑 stride）（**注意：same 不代表输出就跟输出一模一样的形状！**），也就是使得输出的形状为：

$H_2 = \lceil \frac{H_1}{S_h}\rceil, W_2 = \lceil \frac{W_2}{S_w} \rceil$

这其中的秘诀就是如何添加 padding：

如果 $H_1 \% S_h = 0$, $P_h = max(F_h - S_h, 0)$, 否则 $P_h = max(F_h - (H_1 \% S_h), 0)$.

上面的 padding 就是 $P_t = \lfloor \frac{P_h}{2} \rfloor$, 下面的 padding 就是 $P_h - P_t$.

如果 $W_1 \% S_w = 0$, $P_w = max(F_w - S_w, 0)$, 否则 $P_w = max(P_w - (W_1 \% S_w), 0)$.

左边的 padding 就是 $P_l = \lfloor \frac{P_w}{2} \rfloor$, 右边的 padding 就是 $P_w - P_l$.

> 这些 padding 的计算确实比较麻烦，最直接还是直接试一试然后看输出的形状是什么。

#### Normalizations

[参考](https://www.tensorflow.org/addons/tutorials/layers_normalizations)

The basic idea behind these layers is to normalize the output of an activation layer to **improve the convergence during training**.

![image-20210607161038340](%E7%AC%94%E8%AE%B0.assets/image-20210607161038340.png)

#### B-18-IJCAI-Generating Adversarial Examples with Adversarial Networks

图像对抗样本，[复现参考](https://github.com/ctargon/AdvGAN-tf/)

![image-20210607193159778](%E7%AC%94%E8%AE%B0.assets/image-20210607193159778.png)

AdvGAN 的损失函数：

$\mathcal{L} = \mathcal{L}_{adv}^f + \alpha \mathcal{L}_{GAN} + \beta \mathcal{L}_{hinge}$

$\mathcal{L}_{adv}^f = ReLU(max_{i\ne t}f(x_A)_i - f(x_A)_t)$ 其中 $t$ 表示目标标签，$f$ 为目标网络（半白盒场景）或者蒸馏网络（黑盒场景），这个公式来自 A-17-SP-Towards evaluating the robustness of neural networks pp. 6 右边的 $f_2(x^\prime)$，$x_A = x + G(x)$ 就是 perturbed image。

$\mathcal{L}_{GAN}$ 就是 GAN 的 loss，跟 A-14-NIPS-Generative adversarial nets 的 loss （用的 cross entropy）差不多，不过用的是 A-17-ICCV-Least  squares  generative  adversarial  networks 提出的 LSGAN loss （改用了 L2 距离 mean_squared_error，公式 9）。

$\mathcal{L}_{hinge} = ReLU(\lVert G(x) \rVert_2 - c)$ 用于限制 perturbation。

