\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{word2vec}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}NLP with DL}{6}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:nlp}{{1}{6}{NLP with DL}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Word Vector}{6}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An example of word analogy of man:woman :: king:?}}{6}{figure.1.1}\protected@file@percent }
\newlabel{fig:word_analogy}{{1.1}{6}{Word Vector}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Word2vec}{6}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A demo of the window size and $p(w_o | w_c)$}}{7}{figure.1.2}\protected@file@percent }
\newlabel{fig:word2vec:window_cond}{{1.2}{7}{Word2vec}{figure.1.2}{}}
\newlabel{eq:word2vec_loss}{{1.3}{7}{Word2vec}{equation.1.1.3}{}}
\citation{rohde2005_hacks}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}HW1}{9}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}GloVe}{9}{subsection.1.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces An example of co-occurrence matrix with window size of $1$}}{9}{figure.1.3}\protected@file@percent }
\newlabel{fig:cooccurrence_matrix}{{1.3}{9}{GloVe}{figure.1.3}{}}
\citation{GloVe}
\citation{GloVe}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces An example of the conditional probabilities and their ratio in GloVe paper.}}{10}{figure.1.4}\protected@file@percent }
\newlabel{fig:ratio_cooccurrence}{{1.4}{10}{GloVe}{figure.1.4}{}}
\citation{huang-etal-2012-improving}
\citation{TACL_word_senses}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Word sense ambiguity}{11}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Math Backgrounds}{11}{section.1.2}\protected@file@percent }
\citation{Xavier}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Data Preprocessing}{13}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces An exmaple of mean subtration.}}{13}{figure.1.5}\protected@file@percent }
\newlabel{fig:mean_subtraction}{{1.5}{13}{Data Preprocessing}{figure.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces An example of normalization.}}{13}{figure.1.6}\protected@file@percent }
\newlabel{fig:normalization}{{1.6}{13}{Data Preprocessing}{figure.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Parameter Initialization}{13}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Optimizer}{13}{subsection.1.2.3}\protected@file@percent }
\citation{Adam}
\citation{dropout}
\citation{BatchNorm}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces A picture of momentum.}}{14}{figure.1.7}\protected@file@percent }
\newlabel{fig:momentum}{{1.7}{14}{Optimizer}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Regularization}{14}{subsection.1.2.4}\protected@file@percent }
\citation{NER_ICML}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Practice: Named Entity Recognition}{15}{subsection.1.2.5}\protected@file@percent }
\citation{nivre-2003-efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}HW2}{16}{subsection.1.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Dependency Parser}{16}{section.1.3}\protected@file@percent }
\citation{nivre-2003-efficient}
\citation{chen-manning-2014-fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Neural Dependency Parsing}{17}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}HW3}{18}{subsection.1.3.2}\protected@file@percent }
\newpmemlabel{^_1}{18}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Language Modeling and Recurrent Neural Networks}{19}{section.1.4}\protected@file@percent }
\newpmemlabel{^_2}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Principle of RNN}}{19}{figure.1.8}\protected@file@percent }
\newlabel{RNN}{{1.8}{19}{Language Modeling and Recurrent Neural Networks}{figure.1.8}{}}
\citation{RNN_vanishing}
\citation{LSTM}
\citation{GRU}
\citation{ResNet}
\citation{DenseNet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces The repeating module in an LSTM contains four interacting layers.}}{21}{figure.1.9}\protected@file@percent }
\newlabel{fig:LSTM}{{1.9}{21}{Language Modeling and Recurrent Neural Networks}{figure.1.9}{}}
\citation{SMT}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Residual connections.}}{22}{figure.1.10}\protected@file@percent }
\newlabel{fig:ResNet}{{1.10}{22}{Language Modeling and Recurrent Neural Networks}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Dense Net.}}{22}{figure.1.11}\protected@file@percent }
\newlabel{fig:DenseNet}{{1.11}{22}{Language Modeling and Recurrent Neural Networks}{figure.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Seq2Seq and Attention}{22}{section.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Alignment from french to english translation.}}{22}{figure.1.12}\protected@file@percent }
\newlabel{fig:alignment}{{1.12}{22}{Seq2Seq and Attention}{figure.1.12}{}}
\newpmemlabel{^_3}{23}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Training phase for NMT.}}{23}{figure.1.13}\protected@file@percent }
\newlabel{fig:NMT_training}{{1.13}{23}{Seq2Seq and Attention}{figure.1.13}{}}
\newpmemlabel{^_4}{24}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Seq2Seq with attention.}}{24}{figure.1.14}\protected@file@percent }
\newlabel{fig:Seq2Seq_Attention}{{1.14}{24}{Seq2Seq and Attention}{figure.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}HW4}{25}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Contextual Word Representations and Pretraining}{26}{section.1.6}\protected@file@percent }
\newlabel{contextual_word_rep}{{1.6}{26}{Contextual Word Representations and Pretraining}{section.1.6}{}}
\@setckpt{NLP}{
\setcounter{page}{27}
\setcounter{equation}{30}
\setcounter{enumi}{4}
\setcounter{enumii}{2}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{21}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{0}
\setcounter{Item}{23}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{8}
\setcounter{cp@cntr}{4}
\setcounter{NAT@ctr}{0}
\setcounter{@tufte@num@bibkeys}{0}
\setcounter{AM@survey}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{parentequation}{0}
\setcounter{mytheorem}{0}
\setcounter{myproposition}{0}
\setcounter{mydefinition}{0}
\setcounter{mycorollary}{0}
\setcounter{myexample}{0}
\setcounter{myexercise}{0}
\setcounter{Ex}{0}
\setcounter{theorem}{0}
\setcounter{definition}{0}
\setcounter{section@level}{0}
}
