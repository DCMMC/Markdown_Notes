@incollection{word2vec,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {Advances in Neural Information Processing Systems 26},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {3111--3119},
	year = {2013},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@article{rohde2005_hacks,
	title={An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence},
	author={Rohde, Douglas LT and Gonnerman, Laura M and Plaut, David C},
	year={2005}
}

@inproceedings{GloVe,
	title = "{G}love: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	Socher, Richard  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1162",
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}

@inproceedings{huang-etal-2012-improving,
	title = "Improving Word Representations via Global Context and Multiple Word Prototypes",
	author = "Huang, Eric  and
	Socher, Richard  and
	Manning, Christopher  and
	Ng, Andrew",
	booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2012",
	address = "Jeju Island, Korea",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P12-1092",
	pages = "873--882",
}

@article{TACL_word_senses,
	author = {Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and Andrej Risteski},
	title = {Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {6},
	number = {0},
	year = {2018},
	issn = {2307-387X},	pages = {483--495},	url = {https://transacl.org/index.php/tacl/article/view/1346}
}

@inproceedings{NER_ICML,
	author = {Collobert, Ronan and Weston, Jason},
	title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
	year = {2008},
	isbn = {9781605582054},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1390156.1390177},
	doi = {10.1145/1390156.1390177},
	booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	pages = {160–167},
	numpages = {8},
	location = {Helsinki, Finland},
	series = {ICML ’08}
}

@inproceedings{BatchNorm,
	author = {Ioffe, Sergey and Szegedy, Christian},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	year = {2015},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	pages = {448–456},
	numpages = {9},
	location = {Lille, France},
	series = {ICML’15}
}



@inproceedings{nivre-2003-efficient,
	title = "An Efficient Algorithm for Projective Dependency Parsing",
	author = "Nivre, Joakim",
	booktitle = "Proceedings of the Eighth International Conference on Parsing Technologies",
	month = apr,
	year = "2003",
	address = "Nancy, France",
	url = "https://www.aclweb.org/anthology/W03-3017",
	pages = "149--160",
	abstract = "This paper presents a deterministic parsing algorithm for projective dependency grammar. The running time of the algorithm is linear in the length of the input string, and the dependency graph produced is guaranteed to be projective and acyclic. The algorithm has been experimentally evaluated in parsing unrestricted Swedish text, achieving an accuracy above 85{\%} with a very simple grammar.",
}

@inproceedings{chen-manning-2014-fast,
	title = "A Fast and Accurate Dependency Parser using Neural Networks",
	author = "Chen, Danqi  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1082",
	doi = "10.3115/v1/D14-1082",
	pages = "740--750",
}

@article{dropout,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{Adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@InProceedings{Xavier,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@Article{rosenblatt58perceptron,
  author =       {Frank Rosenblatt},
  title =        {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal =      {Psychological Review},
  year =         {1958},
  volume =    {65},
  pages =     {386--408},
  note =      {Reprinted in \emph{Neurocomputing} (MIT Press, 1998)}
}


@Book{mitchell97book,
  author =    {Tom M. Mitchell},
  title =        {Machine Learning},
  publisher =    {McGraw Hill},
  year =         {1997},
}


@InProceedings{brin95nn,
  author =       {Sergey Brin},
  title =        {Near neighbor search in large metric spaces},
  year =      {1995},
  booktitle = {Conference on Very Large Databases (VLDB)}
}



@article{bendavid,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={137},
  year={2007},
}

@article{quinlan,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@inproceedings{ross,
  title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = {St\'ephane Ross and Geoff J. Gordon and J. Andrew Bagnell},
  booktitle = "Proceedings of the Workshop on Artificial Intelligence and Statistics (AIStats)",
  year = {2011}
}

@Unpublished{kaariainen,
  author =       {Matti K{\"a}{\"a}ri{\"a}inen},
  title =        {Lower bounds for reductions},
  note =         {Talk at the Atomic Learning Workshop (TTI-C)},
  month =        {March},
  year =         {2006},
}

@inproceedings{bickel,
  author = {Steffen Bickel and Michael Bruckner and Tobias Scheffer},
  title = {Discriminative Learning for Differing Training and Test Distributions},
  year = {2007},
  booktitle = "Proceedings of the International Conference on Machine Learning (ICML)"
}

@InProceedings{daume,
  author =       {Hal {Daum\'e III}},
  title =        {Frustratingly Easy Domain Adaptation},
  booktitle =    {Conference of the Association for Computational Linguistics (ACL)},
  year =         {2007},
  address =      {Prague, Czech Republic},
}


@article{friedler,
  title={On the (im)possibility of fairness},
  author={Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  journal={arXiv preprint arXiv:1609.07236},
  year={2016}
}


@inproceedings{hardt,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3315--3323},
  year={2016}
}
