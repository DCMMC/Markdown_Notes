{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks\n",
    "\n",
    "卷积神经网络(CNN) 是一种用来处理含有网格状拓扑结构(topology)的数据的神经网络. \n",
    "\n",
    "> 例如时间序列(time series)数据, 可以看做是固定时间间隔上的 1-D 网格; 而图像数据是像素的 2-D 网格.\n",
    "\n",
    "**卷积(Convolutional)** 是一种特殊的线性(数学)操作.\n",
    "\n",
    "> Def.\n",
    ">\n",
    "> 卷积网络是至少有一层被卷积来取代一般矩阵乘法的简单神经网络.\n",
    "\n",
    "因为 Fully Connected MLP 会有大量的 parameters, 这很容易造成过拟合.\n",
    "\n",
    "## convolution operation\n",
    "\n",
    "$$s(t) = \\int x(a) w(t - a) \\mathsf{d}a$$\n",
    "\n",
    "离散情况\n",
    "\n",
    "$$s(t) = \\sum_{a = - \\infty}^\\infty x(a) w(t-a)$$\n",
    "\n",
    "denoted as $s(t) = (x * w)(t)$\n",
    "\n",
    "> 实践中一般都是有限集, 并且 $x$ 一般是多维数组的数据(e.g., 三维张量), $w$ 一般是参数的多维数组, 所在在没有数据的地方都定义函数值为 $0$, 这样无限空间上的积分一般是有限集上的积分, $t$ 也一般是多维的.\n",
    "\n",
    "$s(t)$ 是对 $x(t)$ 在加上概率分布 $w(x)$ 作为权重函数的平滑版的近似. 其中 $x$ 叫做 **输入 input**, 而 $w$ 叫做 **卷积核 convolution kernel 或者 filter  过滤器**, 而输出也叫做 **feature map**.\n",
    "\n",
    "e.g.\n",
    "\n",
    "对于二维图像 $I$ 作为 input, 以及二维 kernel $K$:\n",
    "\n",
    "$$S(i, j) = (I * K) (i,j) = \\sum_m \\sum_n I(m,n)K(i - m, j -n )$$\n",
    "\n",
    "Convolution 是 **可交换 commutative** 的, 所以我们还可以写作\n",
    "\n",
    "$$S(i, j) = (K*I) (i,j) = \\sum_m \\sum_n K(m,n)I(i - m, j -n )$$\n",
    "\n",
    "> [commutative Pf.](http://www.songho.ca/dsp/convolution/convolution_commutative.html) 主要是应用了 kernel 中减号的翻转(flip)\n",
    "\n",
    "一般 library 中实现的是 **cross-correlation** (没有前面的那种 flipped 了)\n",
    "\n",
    "$$S(i,j) = (K*I)(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n)$$\n",
    "\n",
    "> 一般 convolution 都是跟其他函数一起用的, 所以是否 flip 一般不影响结果\n",
    "\n",
    "kernel matrix 一般会限制某些元素等于其他元素, e.g., **Toeplitz matrix** 限制每一行是上一行移动一个元素之后的结果, 二维情况下, doubly circuland matrix 就是一种两重循环的 tensor. [Ref](https://dsp.stackexchange.com/questions/35373/convolution-as-a-doubly-block-circulant-matrix-operating-on-a-vector)\n",
    "\n",
    "> 像前面 kernel 覆盖之外的区域可以视作权重是 $0$ 以及 kernel 限制为 Toeplitz matrix 这些都是 **无限强的先验信念 infinitely strong prior (超级低的熵)**\n",
    "\n",
    "Toeplitz matrix\n",
    "\n",
    "![](./images/cir_matrix.png)\n",
    "\n",
    "下图描述了一个 2-D convolution without kernel flipping\n",
    "\n",
    "![](./images/conv.png)\n",
    "\n",
    "其实 CNN 就是一个 local conv 而不是 fully connected( **简写为 FC层** ) 的 feedforward net.\n",
    "\n",
    "![](./images/local_conv.png)\n",
    "\n",
    "> kernel 一般是比 input 小很多的 **稀疏矩阵 sparse matrix**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv.(convolution 的缩写) 的三个重要思想\n",
    "\n",
    "* 稀疏连接 sparse interactions/sparse connecteivity/sparse weights (反正就是相对于 fully connected)\n",
    "\n",
    "这样可以接受变长输入 inputs of variable size, 并且能够极大地减少空间和时间的消耗.\n",
    "\n",
    "这样做在于, 可能输入图像(e.g.)有百万个像素, 而其中的一条边可能只是几百个像素, 这个可以称为图像的 **局部性原理(Locality)**.\n",
    "\n",
    "某一 layer 中与下一 layer 某 unit 连接的 units 称为这个 unit 的 **接受域 receptive field**, 越深层次的 unit 的 (indirect)接受域就越大, 可以解释为越深层次的抽象能力越高.\n",
    "\n",
    "* 参数共享 parameter sharing/tied weights\n",
    "\n",
    "传统 NN 不会共享 parameters, 在 CNN 中, 因为 kernel matrix 比较小, 所以这一层的所有像素都用着一个 kernel matrix 来处理(跨越多大步长(**stride** )来处理整个图像是一个超参数(就有点像盖章盖满一张白纸, 如何选择每一个章之间的距离就是这个步长, 并且在边界情况下可能盖不全, 这时候可以盖部分或者用 **zero-padding** 用 $0$ 填充), 能够减少空间消耗.\n",
    "\n",
    "如果没有 zero-padding 并且使用 **valid convolution**(matlab 术语(terminology), 下同), 会出现输出 **shrinkage 缩小** 的结果, 这样会限制 conv layer 层数进而限制模型表达\n",
    "\n",
    "> 添加 zero-padding 的地方往往需要更加大一点的 biases\n",
    "\n",
    "![](./images/shrinkage.png)\n",
    "\n",
    "如果加上 zero-padding 就不会出现这种情况, 这种情况叫做 **same convolution**\n",
    "\n",
    "![](./images/zero_padding.png)\n",
    "\n",
    "还有一种 zero-padding 多加的情况, 会导致输出比输入大, 这种情况叫做 **full convolution**, 不过这种会导致 kernel 的优化很难, 不怎么用.\n",
    "\n",
    "\n",
    "* 等变表示 equivariant representation\n",
    "\n",
    "因为参数共享, 所以有等变表示. 这点很有用处, 因为图像中的物体往往可以移动其位置, 这样就算物体移动了位置, 因为参数共享, 我们依然能够得到相同的特征. 不过这种等变表示只适用于移动变换, 如果是缩放或者变形或者旋转变换都没办法处理. 而且这种优势在某些应用上可能会变成劣势, 比如人脸识别中有物体之间的相对位置要求, 比如(一张正放的人物照片), 左右两只眼睛是不一样的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling 池化\n",
    "\n",
    "一层典型的 CNN 层一般由三种 **阶段 stage**(也可以理解为 layer) 组成\n",
    "\n",
    "* Conv. stage 执行 affine transformation\n",
    "* Detector stage 检测阶段, 执行 nonlinearity (e.g. ReLU)\n",
    "* Pooling stage, 将输出元素用它周围的一片局域的 **统计结果 summary statistic** 来替代, e.g. **max polling** 就是以它为中心的一个矩形区域的最大值来代替它, 还有其他诸如求矩形区域的 $L^2$ 范数. 这样的用处: 确保输入的少量变换的结果 **invariant 不变性**, 如果我们需要确保特征并不一定要准确的出现在一个确定的位置的时候, 总的来说就是提高了泛化能力(也这算是一个先验 piror). 跟前面参数共享中说的那个步长 stride 一样, pooling 也可以通过增加步长来 **downsample 减少采样**, 这样可以减少输出量进而减少计算量, \n",
    "\n",
    "stride 为 2 的 pooling\n",
    "\n",
    "![](./images/downsample.png)\n",
    "\n",
    "pooling 可用于接受变长输入, 可以在 pooling 层动态的改变 pooling 大小或者步长大小对不同大小输入产生相同输出就好.\n",
    "\n",
    "pooling 也可以视作是 infinitely strong prior, 这些 convolution 和 pooling 的 prior 如果设置不正确(比如那些先验假设不适用于当前训练数据集), 就很有可能产生 **欠拟合(underfitting)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants of convolution function\n",
    "\n",
    "实际中, 因为一种 convolution function 只能提取一种特征, 如果我们需要在一个 layer 中提取多种特征, 我们就需要多种 conv. operation, 每种负责一部分输入和输出.  而且一般的输入都不是矩阵而是一个有深度 depth (e.g., 图片一般有 RGB 三层信息组成, 所以一般是 width * height * depth 的 tensor) 的张量. 就像下图\n",
    "\n",
    "![](./images/local_conv.png)\n",
    "\n",
    "> tensor 的乘法往往是 reshape 成 matrix 在进行乘法\n",
    "\n",
    "而且一般的软件实现是 **batch 批处理** 而不是单个样本, 所以一般是 4-D tensor(最后一维就是样本的 index)\n",
    "\n",
    "所以 conv 层一般是 multichannel conv., conv. operation 就是对 multichannel 进行处理的而不是原来的一个 channel, 所以 conv. operation 的可交换性就失效了, **除非这个 conv. operation 的输入和输出的 channels 数是一样的** (一般的 ConvNet 也是这样要求的)\n",
    "\n",
    "前面提到的 stride 步长的不同也是一种变种.\n",
    "\n",
    "还有一种情况下我们只想从特定局部位置去提取特征, e.g., 我们要想保证不同区域的眼睛是不一样的(左眼和右眼), 这样可以像 conv 一样使用一个比较小的 kernel matrix, 但是每一局部的 kernel matrix 都不一样(i.e., 不共享), 不会像 convolution 一样 share parameters(kernel matrix), 这种称为 **locally connected layer** 或 **unshare convolution**.\n",
    "\n",
    "![](./images/unshare_conv.png)\n",
    "\n",
    "> 上面的是 unshare conv, 下面的是 conv.\n",
    "\n",
    "还有一种介于 unshare conv. 和 conv. 之间的, 叫做 **Tiled conv.**\n",
    "\n",
    "![](./images/tiled_conv.png)\n",
    "\n",
    "> 上面是 unshare conv., 下面是 tiled conv., tiled conv. 相当于多种 kernel matrix 成块的 conv., 这样相对于 ushare conv. 可以减少空间占用.\n",
    "\n",
    "**Dilated convolutions 膨胀卷积** 进行卷积的时候会有一个间隔的选取元素组成矩阵再乘以卷积核, \n",
    "\n",
    "e.g., in one dimension a filter w of size 3 would compute over input x the following: w\\[0\\]\\*x\\[0\\] + w\\[1\\]\\*x\\[1\\] + w\\[2\\]\\*x\\[2\\]. \n",
    "\n",
    "This is dilation of 0. For dilation 1 the filter would instead compute w\\[0\\]\\*x\\[0\\] + w\\[1\\]\\*x\\[2\\] + w\\[2\\]\\*x\\[4\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "池化层造成的输出变小可以通过 unit stride 解决.\n",
    "\n",
    "## efficient conv. op.\n",
    "\n",
    "用傅立叶变换/reshape变换/近似计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random or unsupervised features\n",
    "\n",
    "可以每层单独的训练, 也可以用 unsupervised 方法去训练出 kernel 甚至用随机值来作为 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: VGG\n",
    "\n",
    "> 物体识别的分类器\n",
    "\n",
    "主要是提出了深度对模型性能的提高。\n",
    "\n",
    "输入为 $224 \\times 224$ 的 RGB 图像, 不过预处理为都减去 RGB各自在整个训练集上的均值(这样的用处应该是为了让差距变大).\n",
    "\n",
    "conv. 的 kernel/filter 全部采用 $3 \\times 3$ 的小卷积核(有利于减少参数数量), 这是最小的有上下左右中间像素的卷积核了, 所以 zero-padding 为 1.\n",
    "\n",
    "conv. 的 stride 步长设置为 1.\n",
    "\n",
    "还有些层使用 $1 \\times 1$ 卷积核, 这可以视作将 input channel 做一个 linear transformation(其实也可以使用 ReLU 之类的非线性).\n",
    "\n",
    "总共用了 5 个 max pooling 层( $2\\times2$ pixel window, stride 2).\n",
    "\n",
    "VGG-16 的结构(有 16 层 weighted-layer):\n",
    "\n",
    "![](./images/VGG16.png)\n",
    "\n",
    "> 最后一个 FC 层 depth 为 1000 是因为训练集有 1000 中类别.\n",
    "\n",
    "![](./images/vgg_config.png)\n",
    "\n",
    "> VGG 的 configuration, 其中 $conv3-64$ 表示 $64$ 种 $3 \\times 3 \\times 3$(最后面这个 $3$ 表示 前面那一 layer channel 数(这里为 RGB 3)) 的卷积核\n",
    "\n",
    "![](./images/channels.png)\n",
    "\n",
    "> 上图就是 conv3-2 的例子\n",
    "\n",
    "用两个 $3 \\times 3$ 级联就可以替换一个 $5 \\times 5$ 的卷积核, 并且每一层 conv. 又会紧接着一个 ReLU($3 \\times3 \\times 2 < 5 \\times 5$ 还可以学习到更多种类的特征(多次重复使用同一大小的卷积核来提取更复杂和更具有表达性的特征)\n",
    "\n",
    "![](./images/5x5.png)\n",
    "\n",
    "前面两个 FC 层采用 **dropout**, dropout 是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率(一般是 Bernoulli 分布)将其暂时从网络中丢弃, dropout 是 CNN 中防止过拟合提高效果的一个大杀器.\n",
    "\n",
    "![](./images/dropout.png)\n",
    "\n",
    "> 相当于添加了一个概率 $r$\n",
    "\n",
    "文中 drop ratio 采用 $0.5$, 0.5 时候 dopout 机生成的网络结构最多.\n",
    "\n",
    "![](./images/dropout_FC.png)\n",
    "\n",
    "那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。\n",
    "\n",
    "dropout 能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力.\n",
    "\n",
    "参数的 $L^2$ 正则设置为 $5 \\times 10^{-4}$.\n",
    "\n",
    "采用 mini-batch SGD, batch 大小设置为 $256$, 学习率 learning rate 设置为 $10^{-2}$, 并且当每次停止上升的时候, 再除以 10.\n",
    "\n",
    "参数的初始化相当重要, 我们首先用 vgg config 中的 model A, 因为它足够浅 shallow, 所以可以用 **随机初始化 random initialization** 的参数(并且学习率在训练中不变), 三层 FC 用 model A 训练出来的参数来初始化, 其他层还是用随机初始化. 所有的随机初始化权重都使用 $\\mathcal{N}(0, 10^{-2})$, 偏置都初始化为 $0$. (后面作者发现全部使用随机初始化也可以...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Algorithm of Artistic Style\n",
    "\n",
    "内容表示和风格表示是分开的.\n",
    "\n",
    "主要原理就是用一张 **白噪声图片 white noise image** 作为 VGG 网络的输入, 然后用反向传播求梯度下降来优化 loss function:\n",
    "\n",
    "$$\\mathcal{L} \\left( \\boldsymbol{p}, \\boldsymbol{a}, \\boldsymbol{x}\\right) = \\alpha \\mathcal{L}_{\\text{content}}(\\boldsymbol{p}, \\boldsymbol{x}) + \\beta \\mathcal{L}_{\\text{style}} (\\boldsymbol{a}, \\boldsymbol{x}) \\\\\n",
    "\\mathcal{L}_{\\text{content}}(\\boldsymbol{p}, \\boldsymbol{x}, l) = \\frac{1}{2} \\lVert F^{(l)} - P^{(l)} \\rVert_F^2 \\\\\n",
    "\\mathcal{L}_{\\text{style}} \\left( \\boldsymbol{a}, \\boldsymbol{x} \\right) = \\sum_{l \\in \\mathbb{L}} \\frac{w_l}{4 N_l^2 M_l^2} \\lVert G^{(l)} - A^{(l)}\\rVert_F^2\n",
    "$$\n",
    "\n",
    "> 为了简略表示, 使用 **Frobenius Norm 矩阵范数**. 并且这两部分 loss function 其实都是 mean-square error\n",
    "\n",
    "其中 $\\boldsymbol{p, a, x}$分别为 原始照片, 画作, 白噪声图片.\n",
    "\n",
    "$P^{(l)}_{i,j}$ $F^{(l)}_{i,j}$ 分别为输入照片和白噪声图片在第 $l$ 层的第 $i$ 个卷积核在 $j$(把二维像素坐标给一维化了) 位置对应的激励值(ReLU 输出值). $N_l$ 和 $M_l$ 分别为第 $l$ 层的卷积核的个数以及平面面积(height $\\times$ width), 内容匹配也是基于物体识别学习出来的内容特征.\n",
    "\n",
    "$A^{(l)}$, $G^{(l)}$ 这两个 Gram matrix 分别为输入风格图片和白噪声图片: $G^{(l)} = \\left[ <F^{(l)}_i,F^{(l)}_j> \\right]$ ($<\\cdot, \\cdot> 表示向量内积$, $F^{(l)}_i,F^{(l)}_j$ 分别为第 $i,j$ 个卷积核的 feature map 的向量化结果). 这一个思想基于 **纹理/风格信息可以用图像局部特征的统计模型 summary statistic 来描述** 的理论, 并且这个局部特征正好是物体识别模型中的表示要获取的, 所以就可以直接套用 VGG 之类的物体识别模型. 并且纹理信息不需要用到整体空间布局, 所以使用同一层 conv. layer 的不同 feature map(卷积核输出的特征)之间的 **相关性 correlation**(i.e., Gram matrix) 来去除全局空间信息保留局部空间信息, 越到物体识别网络的后面几层的时候, 其特征就覆盖范围就越广(接近于全局内容特征)\n",
    "\n",
    "里内容表示的损失函数的超参数 $l$ 设置为 conv4_2, 风格表示的超参数 $\\mathbb{L}$ 设置为 ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’, $w_l$ 统一设置为 $\\frac{1}{5}$, $\\alpha / \\beta$ 通常为 $10^{-3}$ 或 $10^{-4}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
