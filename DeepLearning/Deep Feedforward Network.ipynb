{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feedforward Network 深度前馈网络\n",
    "\n",
    "> 也叫 feedforward neural networks/multilayer preceptrons 多层感知机, 这是一类重要的监督学习网络.\n",
    "\n",
    "目标就是估计一些函数 $f^*(\\boldsymbol{x};\\boldsymbol{\\theta})$, 模型学习参数 $\\boldsymbol{\\theta}$ 来得到最优函数估计，e.g., 对于分类器，$y = f^*(\\boldsymbol{x})$ 就是一个对输入 $\\boldsymbol{x}$ 的映射，映射到类别 $y$.\n",
    "\n",
    "**feedforward** 就是信息流顺序地从 $\\boldsymbol{x}$ 经过 $f$ 中间计算再到输入 $\\boldsymbol{y}$, 而 **feedback 反馈** 就是指模型的输出还会继续回到模型. 同时存在两者的网络称为 **recurrent neural network(复发神经网络)**.\n",
    "\n",
    "网络用有向无环图来描述每个函数之间的复合关系, 例如 $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$, 这是个三层(layer, 又称此网络的深度 depth 为 3)网络, 其中 $f^{(1)}$ 称为第一层, 最后一层叫做输出层; 中间这些就是 **隐藏层 hidden layers**, 因为这些层得出的结果往往还不是想要的结果, 隐藏层中的函数称为 **激活函数 activation function**.\n",
    "\n",
    "我们可以把网络层想象为一堆并行处理的单元组成的, 每一个神经元都是一个 **vector-to-scalar function**.\n",
    "\n",
    "> It is best to think offeedforward networks as function approximation machines that are designed toachieve statistical generalization, occasionally drawing some insights from what weknow about the brain, rather than as models of brain function.\n",
    "\n",
    "线性模型因为是线性函数, 没办法处理不同输入数据之间的交互关系, MLP 正好可以用于解决这样的限制. 其实对线性模型应用 **核方法(kernel trick)**, 达到非线性变换的目的.\n",
    "\n",
    "不过 kernel function 的选择:\n",
    "\n",
    "* 使用一个非常泛化的核函数, 比如高斯核(RBF kernel) 就是映射到一个无限维度空间, 不过过于泛化的话会导致测试结果很糟糕\n",
    "* 手动为每个任务找到最优的核函数, 在深度学习之前这几乎是主流, 不过这种方法的统一性不足, 而且需要花很多的功夫.\n",
    "* 深度学习其实可以说就是用于训练学习得到这个核函数, 不过深度学习没有怎么考虑到问题的凸优化方面(而 SVM/核方法就很注重这个方面), 这是一种短板. 深度学习在可以从一个很宽泛的函数家族来进行训练的基础上, 还可以使用人为的先验知识, 并且先验知识不用像方法 2 那样去选择一个具体的函数, 只需要提供一个偏好的函数家族就行.\n",
    "\n",
    "> 其实使用核函数可以理解一种表示学习方法, 将原始表示映射为其他表示.\n",
    "\n",
    "MLP 学习的是从 $x$ 到 $y$ 的 **确定性映射(deterministic mappings)**, 如果引入 feedback, 模型学习的可以是 **stochastic mappings 随机映射**, 并且还可以得出单个向量的概率分布.\n",
    "\n",
    "除了跟原来线性模型一样需要设计的 **优化方法 optimizer**, **损失函数 cost function** 还有 **输出单元的格式** 之外, MLP 还需要设计 **网络的架构** (e.g., 层数, 各层之间连接性, 还有每层的单元). 深度神经网络需要计算复杂函数的梯度, 到时候我们将使用 **back-propagation 反向传播算法** 及其现代泛化形式用来高效的计算梯度.\n",
    "\n",
    "## Example: Learning XOR\n",
    "\n",
    "原来神经网络提出不就因为没办法实现 XOR 陷入了长达几年的瓶颈.\n",
    "\n",
    "我们可以用原来的最小化 MSE (虽然实际应用中, MSE 不是一个合适的二元数据的损失函数)的线性回归方法来拟合这样的一个定义域只有四个离散值的三维函数, 然而我们会发现拟合出来的函数不好用, 一个解决方案是用类似于核技巧的方法去转换特征空间.\n",
    "\n",
    "> 使用线性模型的时候, 当 $x_1 = 0$ 的时候, 模型输出随着 $x_2$ 增大而增大, 然而当 $x_1 = 1$ 的时候, 模型输出又需要随着 $x_2$ 增大而减小, 所以用一个固定的在 $x_2$ 上的权重没办法做到."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
