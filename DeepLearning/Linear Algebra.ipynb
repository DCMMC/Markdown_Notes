{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $§$ 1.1 Linear Algebra 线性代数\n",
    "\n",
    "> 如果已经熟悉线性代数, 可以跳过本章, 推荐 *The Matrix Cookbook(Petersen and Pedersen, 2006)* 来回顾一些重要公式.\n",
    "> 本章只涉及深度学习需要重要的线性代数知识, 建议参考阅读 *Shilov(1977)* 来全面学习线性代数.\n",
    "\n",
    "> 线性代数是连续数学, 非离散数学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.1 Scalars(标量), Vectors(向量), Matrices(矩阵) 和 Tensors(张量)\n",
    "\n",
    "\n",
    "* 标量一般用小写字母, 斜体表示, 并且一般会指明数的类型, e.g., $s \\in \\mathbb{R}$\n",
    "\n",
    "* 通常向量用小写粗体, 其中的元素写成小写斜体, 如果向量有 $n$ 个元素, 并且所有元素都属于 $\\mathbb{R}$ , 那么该向量属于实数集 $\\mathbb{R}$ 的 n 次 **笛卡尔乘积(Cartesian product)** 构成的集合, 记作(denote) $\\mathbb{R}^{n}$\n",
    "\n",
    "* 如果要明确表示向量中的元素, 将元素排列成一个方括号包围的纵列\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> $x_i$ 是小写斜体, Markdown 里面的内置 $\\LaTeX $ 公式我不知道怎么打小写斜体.\n",
    "\n",
    "* 要取向量 $x$的部分元素作为一个集合, 例如下标 1, 3, 6 的元素, 可以定义 $S = \\{ 1, 3, 6\\}$, 则这个集合表示为 $x_S$. 用符号$-$ 表示集合的补集的索引, 例如 $x_{-1}$ 表示向量 $x$ 中除 $x_1$ 之外的所有元素, 同理 $x_{-S}$.\n",
    "\n",
    "* 矩阵通常用大写粗斜体表示, m行n列并且所有元素都是实数域的矩阵 $A$ 可记作 $A \\in \\mathbb{R}^{m \\times n}$, 对于矩阵元素下标, 可以使用 $:$ 通配符, 例如 $A_{i,:}$ 表示矩阵第 $i$ 行的元素. 里面的元素用大写非粗体.\n",
    "\n",
    "> P.S. \\boldsymbol 太难打了, 所以有些地方只是大写斜体.\n",
    "\n",
    "> $f(A)_{i,j}$ 表示函数 $f$ 应用在 **A** 上输出的矩阵的索引元素.\n",
    "\n",
    "* 张量(Tensors) 就是超过两维的数组, 用大写正粗体字母表示.\n",
    "\n",
    "矩阵的 **转置(transpose)**, 相当于绕着 **主对角线(main diagonal)** 的镜像, 表示为\n",
    "\n",
    "$$\\left(A^{\\top}\\right)_{i,j} = A_{j, i}$$\n",
    "\n",
    "向量就相当于只有一列的矩阵的转置, 有时候就用这种形式表示向量, e.g. $[1,2,3]^{\\top}$.\t标量的转置就等于其本身.\n",
    "\n",
    "矩阵的加减就是把矩阵中的每一个元素进行对应的运算产生一个新的矩阵.\n",
    "\n",
    "在深度学习中, 我们还会使用一种惯用记号, 允许矩阵和向量相加, $C = A + b$ 表示 $C_{i,j} = A_{i, j} + b_j$, 换言之, 矩阵的每一行和向量的转置相加, 这种隐式的复制到很多位置的方式, 被称为 **广播(broadcasting)** .\n",
    "\n",
    "transpose's properties:\n",
    "\n",
    "* $(A + B)^\\top = A^\\top + B^\\top$\n",
    "* $(A^\\top)^{-1} = (A^{-1})^\\top$\n",
    "\n",
    "[More properties](https://en.wikipedia.org/wiki/Transpose#Properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.2 矩阵的乘法\n",
    "\n",
    "$$C = AB$$\n",
    "\n",
    "$$C_{i, j} = \\sum_{k} A_{i, k}B_{k, j}$$\n",
    "\n",
    "\n",
    "> 所以矩阵的标准乘积不是相同位置上元素的乘积(区别于矩阵的加减), 不过矩阵相同位置上的乘积叫做 **元素对应乘积(element-wise product)** 或 **Hadamard 乘积**, 记作 $A \\bigodot B$\n",
    "\n",
    "两个相同维数(dimensionality)的向量的 **点乘(dot product)** 可以看做矩阵乘积 $x \\cdot y = x^{\\top}y$.\n",
    "\n",
    "矩阵的乘法服从分配律(distributive)和结合律(associative):\n",
    "\n",
    "$$A(B + C) = AB + AC$$\n",
    "$$A(BC) = (AB)C$$\n",
    "\n",
    "\n",
    "> 注意, 矩阵乘法运算 $AB$ 和 $BA$ 是不同的, 不符合交换律, 不过向量的点乘符合交换律, $x \\cdot y = y \\cdot x = x^{\\top} y = y^{\\top} x$.\n",
    "\n",
    "$$(AB)^{\\top} = B^\\top A^\\top$$\n",
    "\n",
    "\n",
    "矩阵方程式\n",
    "\n",
    "$$Ax = b$$\n",
    "$$A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, x \\in \\mathbb{R}^n \\text{ is a vector of unknown variables.}$$\n",
    "$$\\Leftrightarrow$$\n",
    "$$A_{1, :} x = b_1$$\n",
    "$$A_{2, :} x = b_2$$\n",
    "$$\\vdots$$\n",
    "$$A_{m, :} x = b_m$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.3 单位矩阵(Identity Matrices)和逆矩阵(Inverse Matrices)\n",
    "\n",
    "**单位矩阵定义**\n",
    "\n",
    "$$I_n \\in \\mathbb{R}^{n \\times n}, \\forall x \\in \\mathbb{R}^n, I_n x = x$$\n",
    "\n",
    "\n",
    "所以单位矩阵就是主对角线的元素都是1, 其他都是0 的矩阵, 例如\n",
    "\n",
    "$$I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "矩阵 $A$ 的逆矩阵记作 $A^{-1}$, 并且有 $A^{-1} A = I_n$\n",
    "\n",
    "所以对于矩阵方程 $Ax = b$ 的解是 $x = A^{-1} b$, 所以这取决于能否找到 $A^{-1}$. 不过用逆矩阵求解矩阵方程主要是一个理论工具, 实际上在软件中使用的不多, 因为在数字计算机中逆矩阵只能表现出有限的精度, 有效使用向量 $b$ 的算法能够获得更精确的 $x$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.4 线性相关(Linear Dependence)和生成子空间(Span)\n",
    "\n",
    "矩阵方程组的解只有 无解, 一个解, 无穷多解 三种.\n",
    "\n",
    "将矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 按列拆分为 $n$ 个列向量(被称为生成空间的 **原点(origin)** ), 则 $A x = \\sum_i x_i A_{:,i}$. 其中 $x_i$ 是标量. $ \\sum_i x_i A_{:,i}$ 就称为 **线性组合(linear combination)** . 一组向量的 **生成子空间(Span)** 就是通过不同的系数($x$ 向量), 所能线性组合出来的所有矩阵(相对于生成空间中的其他所能到达的点)的集合. 这个生成子空间被称为 $A$ 的列空间(column space) 或 $A$ 的值域(range).\n",
    "\n",
    "确定方程组是否有解就是确定 $b$ 是否在 $A$ 的生成子空间中.\n",
    "\n",
    "为了使 $\\forall b \\in \\mathbb{R}^m$,  $Ax = b$ 都有解, 则 $A$ 的列(生成)空间构成整个 $\\mathbb{R}^m$. 所以 $n \\ge m$ 是对每一点有解的必要条件(但不是充分条件). 例如 一个 $3 \\times 2$ 的矩阵的列向量可以看作是一个二维向量, 而 $b$ 却是一个三维向量, 所以 $A$ 的列向量仅仅是 $\\mathbb{R}^3$ 上的一个平面.\n",
    "\n",
    "$n \\ge m$ 仅仅是方程对每一点都有解的必要条件, 因为有可能会出现某些列重复的现象(所以要求所有列线性无关), 例如对于 一个 $2 \\times 2$ 的矩阵, 但是这两列向量是等价的(或者是线性相关的), 则它的列空间是一条直线, 而不能包含整个 $\\mathbb{R}^2$. 如下图:\n",
    "\n",
    "![两个列向量在同一直线上][7]\n",
    "\n",
    "这种冗余的列向量被称为\t**线性相关(linear dependence)**, 如果一组列向量中的任一向量都不能用其他向量的线性组合表示, 则称这组列向量 **线性无关(linear independent)**. 所以如果矩阵的列向量要包含整个 $\\mathbb{R}^m$, 则矩阵必须包含至少一组 $m$ 个线性无关的列向量(注意是至少一组 $m$ 个线性无关的向量而不是至少 $m$ 个线性无关的向量). 这是对方程 $Ax = b$ 中对于任意 $b$ 都有解的 **充要条件** .\n",
    "\n",
    "又因为矩阵必须可逆, 所以矩阵最多只能有 $m$ 个列向量(因为矩阵多于 $m$ 列但是有 $m$ 个线性无关的列向量就会导致对于同一个 $b$ , 有无穷多解).\n",
    "\n",
    "**综上, 矩阵必须是一个方阵(square), 即 $n = m$, 并且所有列向量都线性无关.** \n",
    "\n",
    "> 一个列向量线性相关的方阵被称为 **奇异的(singular)** .\n",
    "\n",
    "如果一个矩阵不是方阵或者是奇异的, 方程组仍然可能有解, 只不过不能使用逆矩阵的方法求解.\n",
    "\n",
    "前面我们讨论的都是逆矩阵左乘, 同样我们可以定义逆矩阵右乘:\n",
    "\n",
    "$$AA^{-1} = I$$\n",
    "\n",
    "对于方阵, 它的左逆(left-inverse)和右逆(right-inverse)是相同的.\n",
    "\n",
    "  [7]: ./images/1516685167266.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.5 范数(Norms)\n",
    "\n",
    "在机器学习中, 我们经常使用范数来度量向量的大小, $L^P$ 范数定义如下:\n",
    "\n",
    "$$\\lVert x \\rVert _p  = \\left( \\sum_i |x_i|^p \\right) ^{\\frac{1}{p}}$$\n",
    "\n",
    "其中 $p \\in \\mathbb{R}, p \\ge 1$.\n",
    "\n",
    "范数(包括 $L^P$ 范数) 是将向量映射到非负值的函数. 直观的来说, 一个向量 $x$ 的范数衡量从原点(origin)到点 $x$ 的距离. 更严格地说, 范数是满足以下条件的任意函数:\n",
    "\n",
    "* $f(x) = 0 \\Rightarrow x = 0$\n",
    "* $f(x + y) \\le f(x) + f(y)$ (三角不等式, the triangle ineuqality)\n",
    "* $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha x) = |\\alpha| f(x)$\n",
    "\n",
    "$L^2$ 范数又称 **欧几里德范数(Euclidean Norm)**, 表示从原点到点 $x$ 的欧几里德距离, 因为使用频繁, 经常把 $\\lVert x \\rVert _2$ 简写成 $\\lVert x \\rVert$ . \n",
    "\n",
    "> 有时候为了方便, 也会使用平方 $L^2$ 范数(也就是去根号的), 不过在靠近原点附近时, 平方 $L^2$ 范数因为增长缓慢不太好用来判断是否非 0, 所以这种情况下还会使用 $L^1$ 范数, i.e., $\\lVert x \\rVert _1 = \\sum_i |x_i|$\n",
    "\n",
    "有时候我们会统计向量中非零个数, 有些作者错误得把它叫做 $L^0$ 范数. $L^1$ 范数经常用来替代表示非零元素数目的函数.\n",
    "\n",
    "另一个经常在机器学习中使用的就是 **$L^{\\infty}$ 范数**, 又被称为 **最大范数** , 用来表示向量中最大幅值的绝对值, $\\lVert x \\rVert _\\infty = \\max_i |x_i|$\n",
    "\n",
    "有时我们可能希望衡量矩阵的大小, 在深度学习中, 最常见的方法是使用 **Frobenius norm** :\n",
    "\n",
    "$$\\lVert A \\rVert _F = \\sqrt{\\sum_{i, j} A_{i, j}^2}$$\n",
    "\n",
    "类似于向量的 $L^2$ 范数. \n",
    "\n",
    "向量的点乘可以写成范数的形式: $x \\cdot y = x^\\top y = \\lVert x \\rVert _2 \\lVert y \\rVert _2 \\cos \\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.6 特殊类型的矩阵和向量\n",
    "\n",
    "**对角矩阵(diagonal matrix)** $diag(v)$ 表示主对角线中元素由向量 $v$ 给定的对角方阵. 对角矩阵的运算很高效, 例如 $diag(v) x = v \\bigodot x$, $diag(v)^{-1} = diag([\\frac{1}{v_1}, \\frac{1}{v_2}, \\cdots , \\frac{1}{v_n}]^\\top)$.\n",
    "\n",
    "> 不是所有的对角矩阵都是方阵, 非方阵的对角矩阵没有逆矩阵.\n",
    "\n",
    "**对称矩阵(symmetric matrix)**\n",
    "\n",
    "$$\\boldsymbol{A}^\\top = \\boldsymbol{A}$$\n",
    "\n",
    "the inverse of the symmetric is also symmetric:\n",
    "\n",
    "$$\\left( \\boldsymbol{A}^{-1} \\right)^\\top = \\left( \\boldsymbol{A}^\\top \\right)^{-1} = \\boldsymbol{A}^{-1}$$\n",
    "\n",
    "**反对称矩阵(anti-symmetric/skew-symmetric matrix)**\n",
    "\n",
    "$$\\boldsymbol{A}^\\top = -\\boldsymbol{A}$$\n",
    "\n",
    "**单位向量(unit vector)** 是具有 **单位范数(unit norm)** 的向量: $\\lVert x \\rVert _2 = 1$.\n",
    "\n",
    "如果 $x ^\\top y = 0$, 那么向量 $x$ 和 $y$ **正交(orthogonal)** , 在 $\\mathbb{R}^n$ 中, 最多有 $n$ 个非零向量互相正交. 如果这些向量不仅正交而且他们都是单位向量, 就叫做 **标准正交(orthonoomal)** .\n",
    "\n",
    "**正交矩阵(orthogonal matrix)** 是行向量和列向量都相互标准正交的矩阵. $A^\\top A = A A^\\top = I$, 也就是说 $A^{-1} = A^{\\top}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.7 特征分解(Eigendecomposition)\n",
    "\n",
    "我们可以通过分解(decompose)矩阵来获得一些用数组表示矩阵不能直观的看出来的函数属性(funtional properties). \n",
    "\n",
    "特征分解是使用最广的一种矩阵分解方法, 将矩阵分解为一组 **特征向量(eigenvectors)** 和 **特征值(eigenvalues)** . \n",
    "\n",
    "**方阵 $A$** 的特征向量是一个非零向量使得 $Av = \\lambda v$. 其中 $\\lambda$ 是一个标量, 也就是特征向量 $v$ 对应的特征值.  \n",
    "\n",
    "> 还可以使用左特征值向量(left eigenvector), $v^\\top A = \\lambda v^\\top$, 但是我们一般使用右特征向量(right eigenvector). \n",
    "\n",
    "如果 $v$ 是 $A$ 的特征向量, 则对于 $sv, s \\in \\mathbb{R}, s \\ne 0$ (s 是标量) 都是 $A$ 的特征向量, 并且他们对应相同的特征值. \n",
    "\n",
    "如果 $A$ 有 $n$ 个线性无关的特征向量 $\\left\\{ v^{(1)}, v^{(2)}, \\cdots, v^{(n)} \\right\\}$ , 和对应的特征值 $\\left\\{ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\right\\}$, 用特征向量按列组成矩阵 $V = [v^{(1)}, v^{(2)}, \\cdots, v^{(n)}]$, 用对应的特征值组成向量 $\\lambda = [\\lambda_1, \\lambda_2, \\cdots, \\lambda_n ]$. 则 $A$ 的 **特征分解** 可以记作\n",
    "\n",
    "$$A = V diag(\\lambda) V^{-1}$$\n",
    "\n",
    "\n",
    "> 不是所有的矩阵都能被分解为特征向量和特征值, 有时候分解是存在的不过是被分解成 **复数(complex)** 而不是实数. 在本书中通常只需要分解一些简单的矩阵.\n",
    "\n",
    "所有的 **实对称矩阵(real symmetric matrix)** 都可以被分解成 **实特征向量(real-valued eigenvectors)** 和 **实特征值(real-valued eigenvalues)** .\n",
    "\n",
    "$A = Q \\Lambda Q^{\\top}$\n",
    "\n",
    "其中 $Q$ 是由 $A$ 的特征向量组成的 **对阵矩阵** , $\\Lambda$ 是对角矩阵, 并且 $\\Lambda_{i, i}$ 是 特征向量 $Q_{:, i}$ 对应的特征值. 我们可以将 $A$ 看作是沿方向 $v^{(i)}$ 延展$\\lambda_i$ 倍的空间.\n",
    "\n",
    "![Figure 1.3][8]\n",
    "\n",
    "> 虽然任意一个实对阵矩阵都可以特征分解, 但是特征分解可能不唯一, 如果多个特征向量共享相同的特征值, 则它们的生成子空间中任意正交的矩阵都可以作为特征向量.\n",
    "\n",
    "按照惯例, $\\Lambda$ 中的元素按照降序排列, 特征分解唯一当且仅当所有的特征值都唯一.\n",
    "\n",
    "矩阵的特征分解提供了很多有用信息:\n",
    "\n",
    "* 方阵是奇异的当且仅当它的特征值都是0.\n",
    "* singular matrix $\\Leftrightarrow$ non-invertible(不可逆矩阵)\n",
    "* 实对称矩阵的特征分解可以用来优化二次方程(二次型, quadratic form) $f(x) = x^\\top A x, \\text{ for } \\lVert x \\rVert _2 = 1$, 如果 $x$ 是 $A$ 的一个特征向量, 那么函数的值就是对应的特征值. $f$ 的最大和最小值分别为矩阵的特征值的最大值和最小值.\n",
    "* 所有特征值都大于 0 的矩阵称为 **正定(positive definite)**, 所有特征值都是非负(也就是有可能为0)的矩阵称为 **半正定(positive semidefinite)**, 所有特征值都是负数的矩阵为 **负定(negative definite)**, 所有特征值都非正(也就是可能为0)的矩阵称为 **半负定(negative semidefinite)**.  对于半正定矩阵, $\\forall x, x^\\top A x \\ge 0$, 正定矩阵还额外保证 $x^\\top A x = 0 \\Rightarrow x = 0$.\n",
    "\n",
    "  [8]: ./images/1516697795762.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.8 奇异值分解(Singular Value Decomposition, *abbr.*, SVD)\n",
    "\n",
    "除了上节提到的特征值分解, 奇异值分解提供了另外一种分解矩阵的方法, 将矩阵分解为 **奇异向量(singular vectors)** 和 **奇异值(singular vaules)**.\n",
    "\n",
    "我们能通过奇异值分解获得与特征分解相同类型的信息, 不过奇异值分解应用的更加广泛, 每个实数矩阵(real matrix) 都有奇异值分解, 但是不一定有特征值分解, 例如如果一个矩阵不是方阵, 我们就只能使用奇异值分解而不能使用特征值分解.\n",
    "\n",
    "使用奇异值可以将矩阵分解为:\n",
    "\n",
    "$$A = U D V^\\top$$\n",
    "\n",
    "如果 $A$ 是一个 $m \\times n$ 矩阵, 则 $U$ 是 $m \\times m$ 矩阵, $D$ 是 $m \\times n$ 矩阵, $V$ 是 $n \\times n$ 矩阵, 其中 $U$ 和 $V$ 都是正交矩阵(酉矩阵(unitary matrix)的特例), $D$ 是一个对角矩阵, 但不一定是方阵.\n",
    "\n",
    "对角矩阵 $D$ 的对角线上的值被称为 $A$ 的 **奇异值(singular values)** , $U$ 和 $V$ 的列向量分别称为 **左奇异向量(left-singular vectors)** 和 **右奇异向量(right-singular vectors)**.\n",
    "\n",
    "可以用 $A$ 相关的特征向量去解释奇异值分解:\n",
    "\n",
    "* $A$ 的左奇异向量就是 $AA^\\top$ 的特征向量\n",
    "* $A$ 的右奇异向量就是 $A^\\top A$ 的特征向量\n",
    "* $A$ 的非零奇异值就是 $AA^\\top$ 的特征值的平方根, 也就是 $A^\\top A$ 的特征值的平方根.\n",
    "\n",
    "**SVD 的最大的一个优点就是将矩阵求逆应用到非方阵上.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.9 Moore-Penrose 伪逆 (The Moore-Penrose Pseudoinverse, aka 广义逆矩阵)\n",
    "\n",
    "对于非方阵, 其逆矩阵没有定义.\n",
    "\n",
    "我们希望对任意的矩阵 $A$ 求解方程 $Ax = y$, 如果 $A$ 的行数大于列数, 则可能无解, 如果列数大于行数, 则可能有多解.\n",
    "\n",
    "Moore-Penrose 伪逆是我们在这种问题上有了一定的进展:\n",
    "\n",
    "$$A^+ = \\lim_{\\alpha \\searrow 0} (A^\\top A + \\alpha I)^{-1} A^\\top$$\n",
    "\n",
    "计算伪逆的实际算法没有使用上面这个定义, 而是使用下面的公式:\n",
    "\n",
    "\n",
    "$$A^+ = V D^+ U^\\top$$\n",
    "\n",
    "其中, $U, D, V$ 是 $A$ 的奇异值分解,  伪逆 $D^+$ 是对 $D$ 对角线上非零元素取倒数然后再转置而成的.\n",
    "\n",
    "* 当 $A$ 的列数大于行数时, 使用伪逆求解线性方程是众多可能的解中的一个, 具体来说, $x = A^+ y$ 是方程所有可行解中欧几里德范数 $\\lVert x \\rVert _2$ 最小的一个\n",
    "* 当 $A$ 的行数大于列数时, 可能没有解. 在这种情况下, 使用伪逆求得欧几里德范数 $\\lVert Ax - y \\rVert_2$ 最小(即 $Ax$ 到 $y$ 的距离最短的)的解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.10 迹(Trace)运算\n",
    "\n",
    "矩阵的迹:\n",
    "\n",
    "$$Tr(A) = \\sum_i A_{i, i}$$\n",
    "\n",
    "有些矩阵运算如果不使用求和公式, 会很难描述, 而通过矩阵乘法和迹运算能够很清楚的表示. 例如, 迹运算提供了另一种描述 Frobenius 范数的方式:\n",
    "\n",
    "$$\\lVert A \\rVert _F = \\sqrt{Tr(AA^\\top)}$$\n",
    "\n",
    "在使用迹运算表示表达式时, 我们可以使用很多有用的等式巧妙得处理表达式. 例如, 迹运算在转置下是不变的: $Tr(A) = Tr(A^\\top)$\n",
    "\n",
    "多个矩阵相乘形成得到的方阵的迹, 就算其中的矩阵位置交换, 也是同样成立的: $Tr(ABC) = Tr(CAB) = Tr(BCA)$\n",
    "\n",
    "而且显然有 $Tr(A - B) = Tr(A) - Tr(B)$, 因为矩阵的加减运算就是对应的每个元素的算术运算.\n",
    "\n",
    "更一般的可以表述成: $Tr\\left( \\prod_{i = 1}^n F^{(i)} \\right) = Tr\\left( F^{(n)} \\prod_{i = 1}^{n - 1} F^{(i)} \\right)$\n",
    "\n",
    "就算 **循环交换** 位置后相乘形成的方阵的形状变化了, 迹还是不变: $Tr(AB) = Tr(BA), \\text{ for } A \\in \\mathbb{R}^{m \\times n}, B \\in \\mathbb{R}^{n \\times m}$.\n",
    "\n",
    "还有一个有用的事实就是对于标量 $a = Tr(a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.11 行列式(The Determinant)\n",
    "\n",
    "方阵 $A$ 的行列式通常表示为 $\\text{det}(A)$, 是一个将方阵映射到实数的函数, 也就是说行列数是一个值. 并且方阵的行列式就是其特征值的乘积. 行列式的绝对值可以用来衡量矩阵参与矩阵乘法之后空间扩了或缩小了多少. 如果行列式为 0 , 则说明空间沿着至少一维完全收缩了, 使其丢失了所有体积(volume); 如果行列式为 1, 则说明这个转换保持体积不变(volume-preserving)\n",
    "\n",
    "Property 1\n",
    "\n",
    "$$|AB| = |A||B|$$\n",
    "\n",
    "Theorem 1\n",
    "\n",
    "For $n \\times n$ matrix $\\boldsymbol{A}$ and its eigenvalues $\\lambda s$\n",
    "\n",
    "$$det(\\boldsymbol{A}) = \\prod_{i=1}^n \\lambda_{i}$$\n",
    "\n",
    "$$tr(A) = \\sum_{i=1}^n \\lambda_i$$\n",
    "\n",
    "> 证明:  [Determinant/Trace and Eigenvalues of a Matrix](https://yutsumura.com/determinant-trace-and-eigenvalues-of-a-matrix/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $§$ 1.1.12 实例: 主成份分析(Principal Components Analysis, *abbr.*, PCA)\n",
    "\n",
    "假设在 $\\mathbb{R}^n$ 空间中有 $m$ 个点 $\\{ x^{(1)}, \\cdots, x^{(m)}\\}$ , 我们需要对它们进行有损压缩(用更少的内存空间但是(尽可能少得)损失精度得保存这些点). \n",
    "\n",
    "一种方式是降维(lower-dimensional), 对于每个 $x^{(i)} \\in \\mathbb{R}$, 都存在一个对应的 **编码向量(corresponding code vector)** $c^{(i)} \\in \\mathbb{R}^l, l < n$ . 即有一个编码函数和一个解码函数使得 $f(x) = \\boldsymbol{c}, x \\approx g(f(x))$.\n",
    "\n",
    "PCA 由我们选择的解码函数定义, 为了使解码简单. 我们使用矩阵乘法来将编码向量映射回原向量, $g(c) = Dc, D \\in  \\mathbb{R}^{n \\times l}$. 为了编码程序简单, PCA 限制 $D$ 的所有列都是互相正交的(不过 $D$ 仍然不是正交矩阵除非 $n = l$). 但是仍然会有多种解, 因为如果我们放大 $D_{:, i}$, 则对应的 $c_i$ 则会缩小, 为了使这个问题只有一个唯一解, 我们限制 $D$ 的每一列向量都有单位范数. \n",
    "\n",
    "为了明确如何根据输入点 $x$ 生成最优编码点(optimal code point) $c^*$, 我们通过找到 $x$ 和 $g(c^*)$ 的最小(欧几里德)距离(为了简化计算, 我们使用平方 $L^2$ 范数):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "c^*& = \\arg\\min_c \\lVert x - g(c) \\rVert _2^2 \\\\\n",
    "& =  \\arg\\min_c (x - g(c))^\\top (x - g(c)) (\\text{ by the definition of } L^2 \\text{ norm.}) \\\\\n",
    "& = \\arg\\min_c [ x^\\top x - x^\\top g(c)  - g(c)^\\top x + g(c)^\\top g(c) ] \\text{ (distributive  property)} \\\\\n",
    "& =  \\arg\\min_c [ x^\\top x - 2 x^\\top g(c) + g(c)^\\top g(c) ] \\text{ (`cause } x^\\top g(c) \\text{ is scalar, and its transpose is itself.)} \\\\\n",
    "& =  \\arg\\min_c [ - 2 x^\\top g(c) + g(c)^\\top g(c) ] \\text{ (omit } x^\\top x \\text{ because it does not depend on } c \\text{ )} \\\\\n",
    "& = \\arg\\min_c [ - 2 x^\\top D c + c^\\top D^\\top Dc ] \\text{ (substitude } g(c) = D c \\text{ )} \\\\\n",
    "& = \\arg \\min_c [ -2 x^\\top D c + c^\\top I_l c ] \\text{ ( } D \\text{ is orthogonal matrix )} \\\\\n",
    "& = \\arg \\min_c [ -2 x^\\top D c + c^\\top c ] \\\\\n",
    "\\end{split}\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "通过 **向量微积分(vector calculus, see Section 1.3.3)** 求解这个最优化问题: \n",
    "\n",
    "$$\\nabla_c ( -2x^\\top D c + c^\\top c) = 0$$\n",
    "$$ -2 D^\\top x + 2c = 0$$\n",
    "$$ c = D^\\top x$$\n",
    "\n",
    "所以编码函数就是: $f(x) = D^\\top x$ (这是一个很高效的算法, 因为我们只需要进行一个矩阵向量运算) . 而 PCA 解码函数就是 $r(x) = g(f(x)) = DD^\\top x$.\n",
    "\n",
    "同样的, 我们通过 **Frobenius 范数** (因为我们要用这一个 $D$ 来编码所有的点(向量))来衡量所有维数和所有点上的误差矩阵的大小: \n",
    "\n",
    "$$D^* = \\arg \\min_D \\sqrt{ \\sum_{i,j} \\left( x_j^{i} - r(x^{(i)})_j \\right)}, D^\\top D = I_l$$\n",
    "\n",
    "\n",
    "> $x^{(i)}$ 是向量, $x_j$ 是向量中的一个元素\n",
    "\n",
    "> $ D^\\top D = I_l$ 是因为 $D$ 的列向量之间互相标准正交.\n",
    "\n",
    "首先我们考虑 $l = 1$ 的情况, 把 $D$ 置换为 $d$, 可以将上式简化为平方 $L^2$ 范数:\n",
    "\n",
    "$$d^* = \\arg \\min_d  \\sum_i  \\lVert x^{(i)} - dd^\\top x^{(i)} \\rVert_2^2 \\text{ , } \\lVert d \\rVert _2 = 1$$\n",
    "\n",
    "又因为 $d^\\top x^{(i)}$ 是一个标量并且标量的转置就是他本身( **这种调整位置的技巧是很有用的** )\n",
    "\n",
    "$$d^* = \\arg \\min_d \\sum_i \\lVert x^{(i)} - x^{(i) \\top} d d \\rVert_2^2  \\text{ , } \\lVert d \\rVert _2 = 1$$\n",
    "\n",
    "对于所有的 $x^{(i)}$, 我们可以按列把它们堆叠在一起作为矩阵进行运算, 这样能紧凑符号. 令 $X \\in \\mathbb{R}^{m \\times n} \\text{, as for } X_{i, :} = x^{(i) \\top}$ , 我们可能将上式表述为** 平方Frobenius 范数**形式:\n",
    "\n",
    "$$d^* = \\arg \\min_d \\lVert X - Xdd^\\top \\rVert _F^2 \\text{, } d^{\\top} d = 1$$\n",
    "\n",
    "\n",
    "> 其中 $Xdd^\\top$ 中的 $d^\\top$ 是因为原来的 $x^{(i)}$ 都作为了 $X$ 的列向量, 原来后面的 $d$ 是将 $x^{(i)}d$ 转化成向量(i.e., 相当于 $\\mathbb{R}^{n \\times 1}$),  而这里的 $d^\\top$ 是将其转化为 $\\mathbb{R}^{1 \\times n}$ 作为矩阵的列向量.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\arg \\min_d  \\lVert X - Xdd^\\top \\rVert _F^2 &= \\arg \\min_d Tr\\left(\\left( X - Xdd^\\top \\right)^\\top \\left( X - X dd^\\top \\right) \\right)\\\\\n",
    "& = \\arg \\min_d [Tr(X^\\top X ) - Tr(X^\\top X d d^\\top) - Tr(dd^\\top X^\\top X) + Tr(dd^\\top X^\\top X d d^\\top)] \\\\\n",
    "& = \\arg \\min_d [  - Tr(X^\\top X d d^\\top) - Tr(dd^\\top X^\\top X) + Tr(dd^\\top X^\\top X d d^\\top) ] \\text{ , remove terms not involving } d\\\\\n",
    "& = \\arg \\min_d [ -2 Tr(X^\\top X d d^\\top ) +  Tr(X^\\top X d d^\\top dd^\\top ) ] \\text{, cycling the order of martices inside trace has same trace }\\\\\n",
    "& = \\arg \\min_d [ -2 Tr(X^\\top X d d^\\top) + Tr(X^\\top X d d^\\top)] \\text{, } d^\\top d = 1\\\\\n",
    "& = \\arg \\min -Tr(X^\\top X d d^\\top) \\\\\n",
    "& = \\arg \\max  Tr( d^\\top X^\\top X d) \n",
    "\\end{split}\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**所以, $d$ 就是 $X^\\top X$ 对应的最大的特征值对应的特征向量, 当 $l >1$ 时, $D$ 由 $X^\\top X$ 前 $l$ 大的特征值对应的特征向量构成(可以用数学归纳法证明, 可作为练习).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
